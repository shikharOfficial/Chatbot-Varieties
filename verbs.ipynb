{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_verbs_entities(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "    \n",
    "    nouns= [token.text for token in doc if token.pos_ == 'NOUN']\n",
    "    \n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    \n",
    "    return { 'verbs': verbs, 'nouns':nouns, 'entities': entities }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title = str(soup.title.string) if soup.title else 'No title'\n",
    "        \n",
    "        # Extract paragraphs\n",
    "        paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "        \n",
    "        paragraphs = list(set(para for para in paragraphs if para))\n",
    "        \n",
    "        para_info = [extract_verbs_entities(para) for para in paragraphs] \n",
    "        title_info =  extract_verbs_entities(title)\n",
    "        \n",
    "        return title, paragraphs, para_info, title_info\n",
    "\n",
    "def process_html_files_recursively(folder_path, output_json_file):\n",
    "    data = []\n",
    "    index = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.html'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                \n",
    "                title, paragraphs, para_info, title_info = extract_content_from_html(file_path)\n",
    "                \n",
    "                data.append({\n",
    "                    'index': index,\n",
    "                    'file_name': file_name,\n",
    "                    'file_path': file_path,\n",
    "                    'title': title,\n",
    "                    'title_info': title_info,\n",
    "                    'paragraphs': paragraphs,\n",
    "                    'para_info': para_info\n",
    "                })\n",
    "                index += 1\n",
    "    \n",
    "    with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(), 'Aurigo_HTML_Files')\n",
    "output_json_file = 'output.json'\n",
    "\n",
    "process_html_files_recursively(folder_path, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
